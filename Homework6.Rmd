---
title: "Homework6"
author: "Dvija Shah"
date: "12/2/2021"
output: 
  pdf_document: 
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Part A
Problems 1–3 ask you to visualize tweets from @realDonaldTrump, as collected by https://www.thetrumpar
chive.com. Download the data files from “twitter.zip” on Piazza.

```{r}
library(tidytext)
library (stringr)
library(dplyr)
library(tokenizers)
library(tidytext)
library(ggplot2)
library(lubridate)
library(glmnet)
```
Problem 1
Import “realDonaldTrump-20201106.csv” making sure that the tweet IDs are preserved. Structure the tweets
into a tidy text format using the token="tweets" tokenizer. Process the data as follows:
•Do not include re-tweets
•Do not include tweets without any spaces
•Remove stop words and “&amp”
•Remove variations on Donald Trump’s name
•Remove URLs and twitter @usernames
Then visualize the top 20 most common terms in Donald Trump’s tweets.

```{r}
DT_tweets <- read.csv("RealDonaldTrump-20201106.csv")
DT_tweets$id <- as.character(DT_tweets$id)
DT_tweets
```
```{r}
DT_tidy <- unnest_tokens(DT_tweets, output="word", input=text, token = "tweets", strip_url =TRUE)
DT_tidy
```
```{r}
DT_tidy <- subset(DT_tidy, isRetweet!= 't') 
DT_tidy
```

```{r}
DT_tidy <- DT_tidy %>%
anti_join(stop_words, by = "word") %>%
filter (!str_detect (word, 'realdonaldtrump')) %>%
filter(!str_detect(word, 'donald')) %>%
filter(!str_detect(word, 'trump'))%>%
filter(!str_detect(word, '[:space:]')) %>%
filter(!str_detect (word, "@")) %>%
filter(!str_detect(word, "https")) %>%
filter(!str_detect(word, "&amp")) %>%
filter (!str_detect (word, "amp"))

DT_tidy
```
```{r}
DT_tidy$Year <- as.Date(DT_tidy$date, format ="%Y")
```


```{r}
DT_tidy$Year<- format(as.Date (DT_tidy$Year, format="%d/%m/%Y"),"%Y")
```

```{r}
DT_tidy
```

```{r}
DT_tidy %>%
count (word, sort=TRUE) %>%
top_n(20) %>%
ggplot(aes (x=reorder(word, n), y=n)) +
geom_col() +
  coord_flip()
labs(x="Word", y="Count", title="Trump Tweets")+
  scale_fill_brewer (palette="Set1") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```
According to the analysis, President, People and Country were the mostly commonly used terms by Donald Trump's in Tweets.




Problem 2
Visualize the top 20 most common terms in Donald Trump’s tweets for each year from 2015-2020, and
comment on the visualization.

```{r}
tidy_trump_2015_2020 <- DT_tidy %>%
filter (between (Year,2015,2020))
tidy_trump_2015_2020
```

```{r fig.width=13, fig.height=8}
tidy_trump_2015_2020 %>%
group_by (Year) %>%
count (word, sort=TRUE) %>%
top_n(20) %>%
ggplot(aes (x=reorder_within(word, n, Year), y=n, fill = Year)) +
geom_col (show.legend = FALSE) +
facet_wrap(.~Year, scales="free") +
coord_flip() +
labs(x="Words", y="Count", title="Trumps Most common words by year")+
    theme(axis.text.y= element_text(angle = 0, hjust = 1, vjust = 0.5))
```
According to the analysis, in the year 2015, President and America were the most commonly used terms in Donald Trump's Tweets.
In 2016, Hillary and People were the most commonly used terms in Donald Trumps Tweets.
In 2017, People, News and Fake were the most commonly used terms in Donald Trumps Tweets.
In2018, People, Country and Democrats were the most commonly used terms in Donald Trumps Tweets.
In 2019, President, People, and Democrats were the most commonly used terms in Donald Trumps Tweets.
In 2020, People, Biden and News were the most commonly used terms in Donald Trumps Tweets.

From this we can analyze that President and People were the most commonly used words in his Presidency Term.
He also mentioned Hillary and Biden in 2016 and 2020 respectively very frequently, as it was at that time
he was running a campaign against them.





Problem 3
Treat year as a “document” to calculate the tf-idf for each term and year. Visualize the top 20 most
characteristic terms in Donald Trump’s tweets for each year from 2015-2020, and comment on the visualization.
```{r}
tweets_tidy_2015_2020_tf_idf<- tidy_trump_2015_2020 %>%
count (Year, word, sort=TRUE) %>%
bind_tf_idf (term=word, document=Year, n=n)

```
```{r fig.width=13, fig.height=8}
tweets_tidy_2015_2020_tf_idf %>%
group_by (Year) %>%
top_n(20, wt=tf_idf) %>%
ggplot (aes (x=reorder_within (word, tf_idf, Year), y=tf_idf, fill=Year)) +
geom_col(show.legend=FALSE)+
theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())+
facet_wrap(~Year, scales="free")+
coord_flip()+labs (y= "Word", y="IF-IDE", title="Top 20 most frequent tweets")+
  scale_x_reordered()+
  theme (axis.text.y = element_text (size=8))
```
Initially in 2015, he mentions alot about the show “#celebapprentice”. In 2016, he started using political
slogans alot and also later in 2020, there was transition in the tweets from politics to coronavirus as that
was the trending topic.



Part B
Problems 4–5 ask you to fit and interpret a model to the tweets analyzed in Part A.

Problem 4
Filter the data to include only tweets from 2016-2020, and then use the glmnet package fit sparse regression
models to predict the number of retweets that a tweet will get. Use cross-validation to select the sparsity
parameter lambda. Report the selected value of lambda and the number of non-zero coefficients in the
regression model. (You do not need to partition the dataset beforehand or report the error.)
Hint: Use the rownames() and colnames() of the sparse matrix to extract the terms and IDs.

```{r}
tweets_dtm<-DT_tidy %>%
filter (Year >= 2016) %>%
count(id, word) %>%
cast_sparse(id, word, n)
tweets_dtm_rows<- tibble(id = rownames (tweets_dtm))
trump_tweets_full <- left_join(tweets_dtm_rows, DT_tweets)
```
```{r}
fitl <- glmnet(tweets_dtm, trump_tweets_full$retweets)
plot(fitl, xvar="lambda", label=TRUE)
```
```{r}
cvfit<- cv.glmnet(tweets_dtm, trump_tweets_full$retweets)
plot(cvfit)
```
```{r}
c1 <- coef (cvfit, s="lambda.min")
sum(c1 != 0)
```
```{r}
c2 <- coef (cvfit, s="lambda.1se" )
sum(c2 != 0)
```
```{r}
print (cvfit)
```
As we can observe from above, lambda lse returns the lesser number of non zero coefficients so we will
select this value to analyze our dataframe of words & their respective coefficients as it has lesser overfitting.
Number of non-zero coefficients can be observed from the print(cvfit) output above.




Problem 5
Extract the coefficients from the best model from Problem 4, and visualize the terms with the strongest
positive relationship with the number of re-tweets. Comment on the visualization.


```{r}
coef_term <- as.data.frame (as.matrix(c2)) %>%
rename (coef = 1) %>%
filter (coef != 0)
arrange (coef_term, desc(coef))
```
The word with the highest correlation to the number of retweets is “#fnn”, followed by “quarantine”. Using
sparse regression we were able to determine that “#fnn” had the highest relationship between a word and
retweet count.


















